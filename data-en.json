{"language":"en","dicts":{"\\en\\getting_started.md":{"title":"\\en\\getting_started.md","path":"\\en\\getting_started.md","markdown":"# Getting Started\r\n\r\n- [Getting Started](#getting-started)\r\n  - [Install and Run](#install-and-run)\r\n    - [Install Dependencies](#install-dependencies)\r\n      - [MacOS](#macos)\r\n      - [ubuntu](#ubuntu)\r\n      - [centos7](#centos7)\r\n      - [archlinux](#archlinux)\r\n    - [Download Pre-compiled Binary](#download-pre-compiled-binary)\r\n    - [Compile From Source Code](#compile-from-source-code)\r\n      - [Get Source Code](#get-source-code)\r\n      - [Install Rust](#install-rust)\r\n      - [Compile](#compile)\r\n    - [Run Single Node Chain](#run-single-node-chain)\r\n    - [Run Multiple Nodes Chain](#run-multiple-nodes-chain)\r\n  - [Use Docker](#use-docker)\r\n  - [Config Reference](#config-reference)\r\n\r\n## Install and Run\r\n\r\nThis is how you can install muta on your own operating system. If you want to try muta via docker quickly, you can check [use docker](#use-docker) first.\r\n\r\n### Install Dependencies\r\n\r\n#### MacOS\r\n\r\n```\r\nbrew install autoconf libtool\r\n```\r\n\r\n#### ubuntu\r\n\r\n```\r\napt update\r\napt install -y git curl openssl cmake pkg-config libssl-dev gcc build-essential clang libclang-dev\r\n```\r\n\r\n#### centos7\r\n\r\n```\r\nyum install -y centos-release-scl\r\nyum install -y git make gcc-c++ openssl-devel llvm-toolset-7\r\n\r\n# enable llvm\r\nscl enable llvm-toolset-7 bash\r\n```\r\n\r\n#### archlinux\r\n\r\n```\r\npacman -Sy --noconfirm git gcc pkgconf clang make\r\n```\r\n\r\n### Download Pre-compiled Binary\r\n \r\nWe will publish the binary files for some common operating system on [github releases](https://github.com/nervosnetwork/muta/releases). \r\nIf your system is in the list, you can download the file directly.\r\n \r\n### Compile From Source Code\r\n\r\n#### Get Source Code\r\n\r\nGet source code via git:\r\n\r\n```\r\ngit clone https://github.com/nervosnetwork/muta.git\r\n```\r\n\r\nOr download the source package on [github releases](https://github.com/nervosnetwork/muta/releases).\r\n\r\n#### Install Rust\r\n\r\nreference： <https://www.rust-lang.org/tools/install>\r\n\r\n```\r\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\r\n```\r\n\r\n#### Compile\r\n\r\n```\r\ncd /path/to/muta\r\nmake prod\r\n```\r\n\r\nThe compiled binary path is `target/release/muta-chain`.\r\n\r\n### Run Single Node Chain\r\n\r\n```\r\ncd /path/to/muta\r\n\r\n# run muta with default config\r\n# if you downloaded the binary instead of compiling it yourself, you should change the path to your binary path\r\n./target/release/muta-chain\r\n\r\n# print help information\r\n$ ./target/release/muta-chain  -h\r\nMuta v0.1.0\r\nMuta Dev <muta@nervos.org>\r\n\r\nUSAGE:\r\n    muta-chain [OPTIONS]\r\n\r\nFLAGS:\r\n    -h, --help       Prints help information\r\n    -V, --version    Prints version information\r\n\r\nOPTIONS:\r\n    -c, --config <FILE>     a required file for the configuration [default: ./devtools/chain/config.toml]\r\n    -g, --genesis <FILE>    a required file for the genesis json [default: ./devtools/chain/genesis.json]\r\n```\r\n\r\n### Run Multiple Nodes Chain\r\n\r\n1. Modify the `config.toml` according to your network topology. Pay attention to the `privkey`, `network` and `verifier_list` part. You can refer the docker-compose config or read the config reference below.\r\n2. Release your binary file, config.toml and genesis.json to you node machines.\r\n3. Start the bootstrap nodes.\r\n4. Start the other nodes.\r\n\r\n## Use Docker\r\n\r\n```\r\ndocker run -it --init -p 8000:8000 nervos/muta\r\n\r\n# if you want to keep the chain data, you can mount the data diretory to the host machine\r\ndocker run -it --init -p 8000:8000 -v `pwd`/data:/app/devtools/chain/data nervos/muta\r\n```\r\n \r\nVisit [graphiql](http://localhost:8000/graphiql) to interact with your chain!\r\n\r\n \r\nUse docker compose to run multiple nodes chain:\r\n\r\n```\r\ndocker-compose -f devtools/docker/dockercompose/docker-compose-bft.yaml up\r\n```\r\n\r\nThis config start 4 nodes, and the chain data path is `target/data/bft1` ~ `target/data/bft4`.\r\nYou can check `docker-compose-bft.yaml` for more details.\r\n \r\n## Config Reference\r\n\r\nThe default config sample is located at `./devtools/chain/config.toml`.\r\nThere are some comments below.\r\n\r\n```toml\r\n# Chain id, the unique identifier of the chain. This field of all nodes of a chain should be the same.\r\nchain_id = \"b6a4d7da21443f5e816e8700eea87610e6d769657d6b8ec73028457bf2ca4036\"  # by sha256(Muta)\r\n\r\n# The private key of the node. \r\n# When connected as the bootstrap node, the public key which the connecting node uses should match this private key.\r\n# When used as validator, the address of this private key should be in the verifier_list.\r\nprivkey = \"45c56be699dca666191ad3446897e0f480da234da896270202514a0e1a587c3f\"\r\n\r\n# Where the chain data locates.\r\ndata_path = \"./devtools/chain/data\"\r\n\r\n[graphql]\r\n# graphql listen address\r\nlistening_address = \"0.0.0.0:8000\"\r\n# graphql path\r\ngraphql_uri = \"/graphql\"\r\n# graphiql path\r\ngraphiql_uri = \"/graphiql\"\r\n\r\n[network]\r\n# p2p listen address\r\nlistening_address = \"0.0.0.0:1337\"\r\n\r\n[[network.bootstraps]]\r\npubkey = \"031288a6788678c25952eba8693b2f278f66e2187004b64ac09416d07f83f96d5b\"\r\naddress = \"0.0.0.0:1888\"\r\n\r\n[mempool]\r\n# The max timeout gap. If current_epoch_id + timeout_gap > timeout field in transaction, the memory pool will reject this transaction.\r\ntimeout_gap = 20\r\n# Memory pool size. When the memory pool is full, new transactions will be rejected.\r\npool_size = 20000\r\n# To increase the performance of memory pool, we broadcast transactions in batches of this number.\r\nbroadcast_txs_size = 200\r\n# Max transaction broadcast interval. Even if there are no transactions more than broadcast_txs_size, they will be broadcasted after this interval.\r\n# The unit is ms.\r\nbroadcast_txs_interval = 200\r\n\r\n[consensus]\r\ncycles_limit = 99999999\r\ncycles_price = 1\r\n# block interval, the unit is ms.\r\ninterval = 3000\r\n# verifier address list\r\nverifier_list = [ \"10f8389d774afdad8755ef8e629e5a154fddc6325a\" ]\r\n\r\n[consensus.duration]\r\n# The numerator of the proportion of propose timeout to the epoch interval.\r\npropose_numerator = 24\r\n# The denominator of the proportion of propose timeout to the epoch interval.\r\npropose_denominator = 30\r\n# The numerator of the proportion of prevote timeout to the epoch interval.\r\nprevote_numerator = 6\r\n# The denominator of the proportion of prevote timeout to the epoch interval.\r\nprevote_denominator = 30\r\n# The numerator of the proportion of precommit timeout to the epoch interval.\r\nprecommit_numerator = 6\r\n# The denominator of the proportion of precommit timeout to the epoch interval.\r\nprecommit_denominator = 30\r\n\r\n[executor]\r\n# When set to true, the node will only keep the latest world state.\r\nlight = false\r\n```"},"\\en\\index.md":{"title":"\\en\\index.md","path":"\\en\\index.md","markdown":"# Muta documentation\r\n\r\n- [Getting Started](./getting_started.md)\r\n-  Module Design \r\n  - [Transaction Pool](./transaction_pool.md)\r\n  - [Overlord Consensus](./overlord.md)\r\n"},"\\en\\overlord.md":{"title":"\\en\\overlord.md","path":"\\en\\overlord.md","markdown":"# Overlord\r\n\r\n## Goal\r\n\r\nOverlord is a Byzantine fault tolerance (BFT) consensus algorithm aiming to support thousands of transactions per second under hundreds of consensus nodes, with transaction delays of no more than a few seconds. Simply put, it is a high-performance consensus algorithm able to meets most of the real business needs.\r\n\r\n## Background\r\n\r\nUsually, a consensus process consists of at least two layers of semantics:\r\n\r\n1. Complete the transaction sequencing \r\n2. Achieve consensus on the latest state\r\n\r\nFor the blockchain of the UTXO model, the new state is implicit in the transaction output, so 1 and 2 are integral and inseparable. For the blockchain of the Account model, the transaction does not contain the state, and only after the transaction is executed can the latest state be generated and the state is saved in an independent MPT tree.\r\n\r\nFor the Account model, in order to implement the second semantic, the common method is to let the consensus nodes execute all the transactions firstly. In this step, the latest state could be calculated and be saved to the block header. And then, consensus node will broadcast the block in which the transaction is sequenced.  Once the consensus is achieved,  the state and the transaction sequence is determined among all the nodes.  \r\nHowever, this method restricts the transaction processing capability of the BFT-like consensus algorithm. As shown in the figure below, Only after the block B(h)(h means the block height) reaches a consensus, the new leader can pack and execute B（h+1），and then broadcast B(h+1). After receiving B(h+1), other consensus nodes need to execute B(h+1) again to verify its correctness. So, in this process, these two serial block execution processes slowed down the consensus efficiency.\r\n\r\n<div align=center><img src=\"./block_process.png\"></div>\r\n\r\nAn improved method is that the Leader does not execute the block immediately after packing the new block. After the block reaches a consensus, the consensus node executes the block to generate a new state, and the next height leader will pack this state and the next height blocks together to participate in the next consensus process. This method saves one block execution process time.\r\n\r\nWhen examining this improvement from a more microscopic perspective, we found that there is still much room for improvement. This is because the consensus module and the execution module of any consensus node are always serial throughout the consensus process. As shown in the figure above, when the consensus module runs, the execution module is always idle, and vice versa. If the execution module and the consensus module can be paralleled, the consensus transaction processing capability can theoretically reach the maximum processing limit of the execution module.\r\n\r\n\r\n\r\n## Overlord Protocol\r\n\r\n### Overview of the protocol\r\n\r\nThe core of Overlord is to decouple transaction sequence and state consensus.\r\n\r\nWe use B(h, S, T) to represent a block of height h, which contains a state of S, and the ordered transaction set is T. In the second semantic of consensus, people's interpretation of S is often the state after the execution of T， and this makes the execution module and consensus module impossible to parallel. If S is understood as the latest state in execution module execution in this moment, the consensus module will not have to wait for the execution module to execute the block. And the execution module only needs to execute forward. In this way, the consensus module can continuously advance, continuously ordering new transactions, and complete consensus on the latest state of the execution module; the execution module can also continuously execute the ordered transaction set until all the ordered transactions are executed.\r\n\r\n\r\n### Protocol description\r\n\r\n\r\nIn Overlord, a consensus process is called an epoch. The epoch contains two parts, Header and Body (as shown below). The core structure of epoch is shown below, `epoch_id` is a monotonically increasing value, equivalent to height; `prev_hash` is the hash of the previous epoch; `order_root` is the merkle root of all pending transactions contained in the Body; `state_root` represents the latest world The MPT root of the state; `confirm_roots` represents the `order_root` collection from the `state_root` of the previous epoch to the `state_root` of the current epoch; the `receipt_roots` records the `receipt_root` corresponding to each `order_root` being executed; proof is proof of the previous epoch .\r\n\r\n<div align=center><img src=\"./epoch.png\"></div>\r\n\r\nIn this method, the consensus module batches the transactions to make a consensus. After the consensus is reached, the ordered transaction set is added to the queue to be executed, and the execution module executes in order of the transaction set, and each execution of the transaction set is performed. The ordered root of the transaction set to be executed, and the executed stateRoot are sent to the consensus module. When packing the transactions to assemble the epoch, the leader take the latest state_root as the latest state to participate in the consensus.\r\n\r\nOverlord is an interpretation layer above the specific consensus algorithm. By reinterpreting the semantics of consensus, the transaction sequence is decoupled from the state consensus, so that higher transaction processing capability can be obtained in actual operation. In theory, Overlord can be based on almost any BFT-like consensus algorithm, specifically in our project based on the improved Tendermint.\r\n\r\nWe have made three major improvements compared to Tendermint:\r\n\r\n1. Apply the aggregate signature to Tendermint to make the consensus message complexity from <img src=\"https://latex.codecogs.com/svg.latex?\\inline&space;O(n^{2})\" title= \"O(n^{2})\" /> falls to <img src=\"https://latex.codecogs.com/svg.latex?\\inline&space;O(n)\" title=\"O(n)\" />, thus being able to support more consensus nodes\r\n2. The propose transaction area is added to the proposal, so that the synchronization of the new transaction can be paralleled with the consensus process.\r\n3. After receiving the proposal, the consensus node can vote for the prevote without waiting for the epoch check, and must obtain the epoch check result before voting the precommit vote, so that the block check is parallel with the prevote process.\r\n\r\n#### Aggregate signature\r\n\r\nIn the Tendermint consensus protocol, the node casts a prevote on the proposal after it receives the proposal, and the prevote vote is broadcast to other nodes throughout the network. The communication complexity at this time is <img src=\"https://latex.codecogs.com/svg.latex?\\inline&space;O(n^{2})\" title=\"O(n^{2})\" /> Using aggregated signature optimization is the process by which all nodes send prevote votes to a specified Relayer node, which can be any consensus node. The Relayer node calculates the aggregated signature by the algorithm, and then uses a bitmap (bit-vec) to indicate which nodes vote. Send aggregated signatures and bitmaps to other nodes, for the same reason as precommit voting. This reduces the communication complexity to <img src=\"https://latex.codecogs.com/svg.latex?\\inline&space;O(n)\" title=\"O(n)\" />.\r\n\r\nIf Relayer fails, no aggregated signature is sent to the consensus node, or Relayer does evil, and only a small number of consensus nodes send aggregated signatures, the consensus will be inactivated. We use a time-out vote to solve this problem. When the node sends a prevote vote, it immediately sets a timer. If the prevoteQC is not received within the timeout period, it directly enters the pre-commit status, and the nil precommit vote is thrown. Then go to the next round. If the pre-voting phase is normal, a timer is also set after the precommit is sent. If the precommitQC is not received after the timeout, the next round is entered.\r\n\r\n#### Synchronous parallelism\r\n\r\nOverlord broadcasts CompactEpoch in a compact block, meaning that its body contains only transaction hashes, not full transactions. After receiving the CompactEpoch, the consensus node needs to synchronize all the complete transactions contained in its Body to construct a complete epoch.\r\n\r\nIn addition to the CompactEpoch, we also added a propose trading area in the proposal. The pose contains the hash of the new transaction to be synchronized. It should be noted that these transactions do not overlap with the pending transaction hashes contained in CompactEpoch. When CompactEpoch is not sufficient to contain all new transactions in the trading pool, the remaining new transactions can be included in the proposed trading area for early synchronization. This can increase the degree of concurrency of transaction synchronization and consensus when the system transaction volume is large, and further improve transaction processing capability.\r\n\r\n#### Verify parallelism\r\n\r\nAfter receiving the *proposal*, the consensus node will verify the *CompactEpoch* (to obtain the complete transaction and verify the correctness of the transaction) in parallel with the *prevote* vote. Only after receiving the *prevote* aggregate signature and the *CompactEpoch* test result will the *precommit* be cast.\r\n\r\n## Overlord architecture\r\n\r\nThe Overlord consensus consists of the following components:\r\n\r\n* State Machine (SMR): State transition based on input messages\r\n* State storage (State): used to store the status of the proposal, voting, etc.\r\n* Timer: Set the timeout period to trigger the state machine operation\r\n* Wal: used to read and write Wal logs\r\n\r\nIn the Overlord consensus architecture, when a message is received, the state storage module performs a basic check on the message. After passing, the status is updated according to the received message and the message is transmitted to the state machine. In addition, a timer is required to maintain activity, and when the timer expires, the timer calls the interface to trigger the state machine. The state machine will throw a current state event after making the state change. The state storage module and the timer module listen to the event thrown by the state machine, and perform corresponding processing according to the monitored event, such as writing Wal, sending a vote, setting the timing. And so on. At the time of the restart, the state storage module reads the data from the Wal and sends it to the state machine. The overall architecture is shown below:\r\n\r\n<div align=center><img src=\"./arch_overlord.png\"></div>\r\n\r\n### Consensus State Machine (SMR)\r\n\r\nThe state machine module is the logical core of the entire consensus, and its main functions are state changes and *lock* control. When the received message is triggered, the status change is made according to the received message, and the changed status is thrown as an event. In our implementation, Overlord uses a Tendermint state machine that applies BLS aggregate signature optimization for consensus. The overall working process is as follows.\r\n\r\n#### Prepare phase\r\n\r\nThe node uses a deterministic random algorithm to determine the leader of the current round.\r\n\r\n*Leader*: Broadcast a proposal\r\n\r\n*Others*: Set a timer T1 to send a *prevote* vote to *Relayer* when the proposal is received\r\n\r\n#### Prevote step\r\n\r\n*Relayer*: Set a timer T2 to aggregate the received *prevote* votes and generate a bitmap to broadcast the aggregated votes and bitmaps to other nodes.\r\n\r\n*Others*: Set a timer T2, check the validity of the aggregated prevote vote, generate *PoLC* send precommit vote\r\n\r\n####Verification step\r\n\r\nAll nodes set a timer T3. After receiving the verification result of the *proposal*, they enter the *pre-commit* stage.\r\n\r\n#### Precommit step\r\n\r\n*Relayer*: Set a timer T4 to aggregate the received precommit votes and generate a bitmap to broadcast the aggregated votes and bitmaps to other nodes.\r\n\r\n*Others*: Set a timer T4 to check the legitimacy of the aggregated precommit vote\r\n\r\n#### Commit step\r\n\r\nAll nodes commit the proposal\r\n\r\nThe state transition diagram of the consensus state machine is shown below:\r\n\r\n<div align=center><img src=\"./state_transition.png\"></div>\r\n\r\nIn the project, we combine the pre-voting phase and the verification phase into one phase, sharing a timeout. When the state machine receives the aggregated voting and verification results, it enters the pre-commit phase.\r\n\r\n#### State Machine State\r\n\r\nThe states that the state machine module needs to store are:\r\n\r\n* epoch_id: current consensus epoch\r\n* round: round of current consensus\r\n* step: the current stage\r\n* proposal_hash: optional, current hash of consensus\r\n* lock: optional, whether it has been reached *PoLC*\r\n\r\n#### data structure\r\n\r\nThe trigger structure of the state machine is as follows:\r\n\r\n```\r\npub struct SMRTrigger {\r\n    pub hash: Hash,\r\n    pub round: Option<u64>,\r\n    pub trigger_type: TriggerType,\r\n}\r\n```\r\n\r\nThe output structure of the state machine is as follows:\r\n\r\n```\r\npub enum SMREvent {\r\n    /// New round event\r\n    /// for state: update round,\r\n    /// for timer: set a propose step timer.\r\n    NewRoundInfo {\r\n        round: u64,\r\n        lock_round: Option<u64>,\r\n        lock_proposal: Option<Hash>,\r\n    },\r\n    /// Prevote event,\r\n    /// for state: transmit a prevote vote,\r\n    /// for timer: set a prevote step timer.\r\n    PrevoteVote(Hash),\r\n    /// Precommit event,\r\n    /// for state: transmit a precommit vote,\r\n    /// for timer: set a precommit step timer.\r\n    PrecommitVote(Hash),\r\n    /// Commit event\r\n    /// for state: do commit,\r\n    /// for timer: do nothing.\r\n    Commit(Hash),\r\n    /// Stop event,\r\n    /// for state: stop process,\r\n    /// for timer: stop process.\r\n    Stop,\r\n}\r\n```\r\n\r\n#### State Machine Interface\r\n\r\n```\r\n/// Create a new SMR service.\r\npub fn new() -> Self\r\n/// Trigger a SMR action.\r\npub fn trigger(&self, gate: SMRTrigger) -> Result<(), Error>\r\n/// Goto a new consensus epoch.\r\npub fn new_epoch(&self, epoch_id: u64) -> Result<(), Error>\r\n```\r\n\r\n### State storage (State)\r\n\r\nThe state storage module is the functional core of the entire consensus. The main functions are storage state, message distribution, block, and cryptography related operations. In the working process, for the message transmitted by the network layer, the first check is performed to verify the validity of the message. Determine whether the written message needs to be written to Wal. The message is then sent to the state machine. The state storage module constantly listens for events thrown by the state machine and processes them accordingly.\r\n\r\n#### Storage Status\r\n\r\nThe state that the state storage module needs to store include:\r\n\r\n* epoch_id: current consensus epoch\r\n* round: round of current consensus\r\n* proposals: cache current epoch all offers\r\n* votes: cache current epoch all votes\r\n* QCs: Cache current epoch all QC\r\n* authority_manage: consensus list management\r\n* is_leader: whether the node is a leader\r\n* proof: optional, proof of the last epoch\r\n* last_commit_round: optional, the last round of submissions\r\n* last_commit_proposal: Optional, last submitted proposal\r\n\r\n#### Message Distribution\r\n\r\nWhen sending a message, choose how to send the message based on the message and parameters (broadcast to other nodes or sent to Relayer).\r\n\r\n#### Block\r\n\r\nWhen the state storage module listens to the NewRound event thrown by the state machine, it determines whether it is a block node by a deterministic random number algorithm. If it is a block node, a proposal is made.\r\n\r\nDeterministic random number algorithm: Because the Overlord consensus protocol allows different out-of-block weights and voting weights to be set, when determining the block, the node normalizes the block weights and projects them into the entire u64 range, using the current epoch_id and The sum of round is used as a random number seed to determine which of the u64 ranges the generated random number falls into, and the node corresponding to the weight is the outbound node.\r\n\r\n#### Cryptography\r\n\r\nCryptographic operations include the following methods:\r\n\r\n* When the message is received, the signature of message need to be verified\r\n* When receiving the aggregate vote, verify the signature and check whether the weight exceeds the threshold\r\n* Sign the message when making a proposal or voting\r\n* When you are a Relayer, aggregate the votes you receive.\r\n\r\n#### Status Storage Interface\r\n\r\n### Timer\r\n\r\nWhen the state machine runs to certain states, it needs to set a timer to perform operations such as timeout retransmission. The timer module listens for events thrown by the state machine and sets the timer based on the event. When the timeout period is reached, the interface of the calling state machine module triggers a timeout. The timer is multiplexed with the state store SMREvent and interface.\r\n\r\n### Wal\r\n\r\nIn the consensus process, some messages need to be written to Wal. When restarting, the state storage module first reads the message from Wal and replies to the state before the restart. The Wal module only interacts with the state storage module.\r\n\r\n#### Wal Interface\r\n\r\n```\r\n/// Create a new Wal struct.\r\npub fn new(path: &str) -> Self\r\n/// Set a new epoch of Wal, while go to new epoch.\r\npub fn set_epoch(&self, epoch_id: u64) -> Result<(), Error>\r\n/// Save message to Wal.\r\npub async fn save(&self, msg_type: WalMsgType, msg: Vec<u8>) -> Result<(), Error>;\r\n/// Load message from Wal.\r\npub fn load(&self) -> Vec<(WalMsgType, Vec<u8>)>\r\n```\r\n\r\n## Overlord Interface\r\n\r\n### Consensus Interface\r\n\r\n```\r\n#[async_trait]\r\npub trait Consensus<T: Codec>: Send + Sync {\r\n    /// Get an epoch of an epoch_id and return the epoch with its hash.\r\n    async fn get_epoch(\r\n        &self,\r\n        _ctx: Vec<u8>,\r\n        epoch_id: u64,\r\n    ) -> Result<(T, Hash), Box<dyn Error + Send>>;\r\n\r\n    /// Check the correctness of an epoch. If is passed, return the integrated transcations to do\r\n    /// data persistence.\r\n    async fn check_epoch(\r\n        &self,\r\n        _ctx: Vec<u8>,\r\n        epoch_id: u64,\r\n        hash: Hash,\r\n    ) -> Result<(), Box<dyn Error + Send>>;\r\n\r\n    /// Commit a given epoch to execute and return the rich status.\r\n    async fn commit(\r\n        &self,\r\n        _ctx: Vec<u8>,\r\n        epoch_id: u64,\r\n        commit: Commit<T>,\r\n    ) -> Result<Status, Box<dyn Error + Send>>;\r\n\r\n    /// Get an authority list of the given epoch ID.\r\n    async fn get_authority_list(\r\n        &self, \r\n        _ctx: Vec<u8>, \r\n        epoch_id: u64\r\n    ) -> Result<Vec<Node>, Box<dyn Error + Send>>;\r\n\r\n    /// Broadcast a message to other replicas.\r\n    async fn broadcast_to_other(\r\n        &self,\r\n        _ctx: Vec<u8>,\r\n        msg: OutputMsg<T>,\r\n    ) -> Result<(), Box<dyn Error + Send>>;\r\n\r\n    /// Transmit a message to the Relayer, the third argument is the relayer's address.\r\n    async fn transmit_to_relayer(\r\n        &self,\r\n        _ctx: Vec<u8>,\r\n        addr: Address,\r\n        msg: OutputMsg<T>,\r\n    ) -> Result<(), Box<dyn Error + Send>>;\r\n}\r\n```\r\n\r\n### Cryptography Interface\r\n\r\n```\r\npub trait Crypto {\r\n    /// Hash a message.\r\n    fn hash(&self, msg: &[u8]) -> Hash;\r\n\r\n    /// Sign to the given hash by private key.\r\n    fn sign(&self, hash: Hash) -> Result<Signature, Box<dyn Error + Send>>;\r\n\r\n    /// Aggregate signatures into an aggregated signature.\r\n    fn aggregate_signatures(\r\n        &self,\r\n        signatures: Vec<Signature>,\r\n    ) -> Result<Signature, Box<dyn Error + Send>>;\r\n\r\n    /// Verify a signature.\r\n    fn verify_signature(\r\n        &self,\r\n        signature: Signature,\r\n        hash: Hash,\r\n    ) -> Result<Address, Box<dyn Error + Send>>;\r\n    \r\n    /// Verify an aggregated signature.\r\n    fn verify_aggregated_signature(\r\n        &self,\r\n        aggregate_signature: AggregatedSignature,\r\n    ) -> Result<(), Box<dyn Error + Send>>;\r\n}\r\n```"},"\\en\\transaction_pool.md":{"title":"\\en\\transaction_pool.md","path":"\\en\\transaction_pool.md","markdown":"# Mempool\r\n\r\n## Design Requirements\r\n\r\nMempool is responsible for collecting and packaging new transactions to consensus module.\r\nNaturally, we have some requirements for mempool.\r\n\r\n1. Excellent performance. \r\nIt is required to achieve the performance of inserting 10,000+ tps in an ordinary computer.\r\n\r\n2. Fairness. Transactions should been packaged in the order in which they are received.\r\n\r\n3. In addition, in order to match the design of PPCT (parallel process of consensus \r\nand transaction-synchronization), there is a third requirement.\r\nThe data structure returned by package should consists of two parts: \r\norder-transactions for consensus and propose-transactions for synchronization.\r\n\r\n## Solution\r\n\r\n### Requirement 1.\r\n\r\nTo achieve excellent performance, we should first analyze the process of insertion and \r\npropose solutions to the performance bottlenecks. \r\nThe process includes:\r\n1. Check if the mempool is full.\r\n2. Check if the transaction has been included in the mempool.\r\n3. Check that the signature of the transaction is correct and the format is compliant.\r\n4. Check if the transaction is already on the chain.\r\n\r\nSteps 1, 2 are very fast, not a performance bottleneck.\r\n\r\nStep 3 involves the verification of signature, which is a time-consuming operation. \r\nFortunately, the verification is an independent computationally intensive operation. \r\nIt is suitable for high-concurrency to fully exploit CPU performance to improve performance.\r\n\r\nAs the blockchain continues to grow, historical transaction data is growing, \r\nand the query in step 4 will become a performance black hole. \r\nWe solve this problem by setting a timeout field in the transaction \r\nand a global constraint parameter `g`.\r\n\r\nSpecifically, when the timeout of a transaction is `t`, if this transaction is still unpacked at the \r\nheight of `t`, it will be considered invalid and discarded by the mempool. \r\nIn order to avoid users setting timeout too high, if the timeout `t` > `h` + `g`, \r\nmempool with latest height `h` will also discard such transaction as illegal. \r\nUnder this constraint, mempool with latest height `h` only needs \r\nto keep historical transactions with height between [h - g, h] for checking, \r\nand do the computational and storage complexity of the check are both reduced to `O(g)`, \r\nregardless of the total amount of historical transactions.\r\n\r\n### Requirement 2.\r\n\r\nIn the case of the same transaction priority, if transactions received after but package first, \r\nthis is obviously against fairness. \r\nTherefore, transactions in the mempool must be packaged on a FIFO basis.\r\n\r\nHowever, according to Ethereum's nonce monotonous design, \r\nif the transaction pool contains multiple transactions issued by the same user, \r\nthen the transaction needs to satisfy the partial order relationship, \r\nwhich brings great complexity to the packaging design. \r\nTherefore, we take random nonce instead. \r\nThis design also brings additional benefits, \r\nsuch as better concurrent execution, simplified wallet design, and more.\r\n\r\nIn short, it is unnecessary and inefficient to force all transactions of a user to remain partial order. \r\nIf there is some dependency between certain transactions, \r\nwe can use a `ref` field to represent this relationship. \r\nThis is a generic dependency expression compared to Ethereum. \r\nOur package algorithm can be easily extended to meet this dependency requirement.\r\n\r\n### Requirement 3.\r\n\r\nSince blockchain is a distributed system, \r\nthe transaction set of different node's mempool will not be identical.\r\nThe core idea of PPCT is that if there are so many transactions in the mempool that \r\ncannot reach consensus in one time, \r\nthe left transactions can be synchronized in parallel with the consensus process.\r\nWith this design, the synchronization process of the order-transactions \r\ncan started one epoch earlier, such that the consensus efficiency has been improved.\r\n\r\nSpecifically, at the time of packaging, after order-transaction been full, \r\nmempool continues to package transactions as propose-transaction.\r\nThe proposal issued by the leader contains order-transactions and propose-transactions. \r\nThe order-transaction participates in the consensus, and the propose-transaction begins to synchronize.\r\n\r\n## Specific Design\r\n\r\nAccording to the above analysis, what we need is a mempool that can support high concurrent insertion, \r\npackage transactions on a FIFO basis, and package two types of transactions with different purposes.\r\n\r\nIn order to meet the above requirements, we use the Map and Queue structures to share transactions received, \r\nMap can quickly query and delete, and Queue meets the FIFO packaging requirements. \r\nIn fact, we used two queues, just like pouring milk in two cups. \r\nThe core data structure of mempool is as follows.\r\n\r\n```rust\r\nstruct TxCache {\r\n    /// Use two queues to store transactions in turn. \r\n    /// One queue for incumbent, and the other is candidate.\r\n    /// Insertion and Packaging are working on incumbent queue.\r\n    queue_0: Queue<SharedTx>,\r\n    queue_1: Queue<SharedTx>,\r\n    /// Use map to complete efficient random queries and deletion.\r\n    map: Map<Hash, SharedTx>,\r\n    /// Indicate which queue is incumbent.\r\n    is_zero: AtomicBool,\r\n    /// Used for atomic operations to properly handle concurrency issues.\r\n    concurrent_count: AtomicUsize,\r\n}\r\n\r\n/// A structure for sharing transactions in map and queues.\r\ntype SharedTx = Arc<TxWrapper>;\r\n\r\nstruct TxWrapper {\r\n    tx: SignedTransaction,\r\n    /// Indicate whether this transaction is deleted by map.\r\n    removed: AtomicBool,\r\n    /// Indicate whether this transaction is from propose-transaction synchronization.\r\n    proposed: AtomicBool,\r\n}\r\n\r\n/// Store transactions that from order-transaction synchronization.\r\ntype CallbackCache = Map<Hash, SignedTransaction>;\r\n\r\n/// Data structure returned by package.\r\nstruct MixedTxHashes {\r\n    order_tx_hashes: Vec<Hash>,\r\n    propose_tx_hashes: Vec<Hash>,\r\n}\r\n```\r\n\r\nNew transactions that pass all checks are wrapped as `TxWrapper` with `removed` and `proposed` \r\nset to `false`, and then convert to `SharedTx` and insert into `TxCache`.\r\n\r\n`MixedTxHashes` is the data structure returned by package, which contains `order_tx_hashes` for consensus \r\nand `propose_tx_hashes` for early synchronization.\r\n\r\nThe package algorithm is as follows, popping transactions from the incumbent queue, \r\nskipping the `TxWrapper` of `removed = true` until reach `cycle_limit`, \r\nand these transaction hashes are as `order_tx_hashes`. \r\nContinue to pop transactions, skipping the `TxWrapper` of `proposed = true` until reach `cycle_limit`, \r\nand these transaction hashes are as `propose_tx_hashes`. \r\nThe above pop-up transactions are pushed into the candidate queue except for transactions of `removed = true`. \r\nWhen the incumbent queue is popped up, the role of two queues are exchanged.\r\n\r\nWhen a node receives a proposal from the leader, \r\nit will query mempool checking `order_tx_hashes` and `propose_tx_hashes`. \r\nMempool determines if a transaction exists by querying `TxCache.map` \r\nand initiates a synchronization request for the missing transaction. \r\nThe order-transaction returned synchronously is inserted into `CallbackCache`, \r\nand the propose-transaction returned for synchronization is inserted into `TxCache` \r\nwith `proposed` setting `true`.\r\n\r\nWhen mempool receives a request to delete a set of `tx_hashes`, first clear `CallbackCache`, \r\nthen query `TxCache.map`, set `removed` in the corresponding `TxWrapper` to `true`, \r\nand then delete the `SharedTx`.\r\n\r\nThe insertion and packaging process of mempool is shown in the figure below.\r\n\r\n![image](./resources/mempool_process_en.png)\r\n"}},"menus":"<h1 id=\"muta-documentation\">Muta documentation</h1>\n<ul>\n<li><a href=\"./getting_started.md\">Getting Started</a></li>\n<li>Module Design</li>\n<li><a href=\"./transaction_pool.md\">Transaction Pool</a></li>\n<li><a href=\"./overlord.md\">Overlord Consensus</a></li>\n</ul>\n","default":""}